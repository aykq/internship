{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa9Z89HYBqQ3f7NqPJpX0s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aykq/internship/blob/main/tensorflow/04_Transfer_Learning_with_TensorFlow_Part_2_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with TensorFlow Part 2: Fine-tuning"
      ],
      "metadata": {
        "id": "Lwn0ugA-Hxr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we saw how we could leverage feature extraction transfer learning to get far better results on our Food Vision project than building our own models (even with less data).\n",
        "\n",
        "Now we're going to cover another type of transfer learning: fine-tuning.\n",
        "\n",
        "In **fine-tuning transfer learning** the pre-trained model weights from another model are unfrozen and tweaked during to better suit your own data.\n",
        "\n",
        "For feature extraction transfer learning, you may only train the top 1-3 layers of a pre-trained model with your own data, in fine-tuning transfer learning, you might train 1-3+ layers of a pre-trained model (where the '+' indicates that many or all of the layers could be trained).\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-transfer-learning-feature-extraction-vs-fine-tuning.png)\n",
        "\n",
        "Feature extraction transfer learning vs. fine-tuning transfer learning. The main difference between the two is that in fine-tuning, more layers of the pre-trained model get unfrozen and tuned on custom data. This fine-tuning usually takes more data than feature extraction to be effective."
      ],
      "metadata": {
        "id": "vxfTNGJdH6tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What we're going to cover\n",
        "\n",
        "We're going to go through the follow with TensorFlow:\n",
        "+ Introduce fine-tuning, a type of transfer learning to modify a pre-trained model to be more suited to your data\n",
        "+ Using the Keras Functional API (a differnt way to build models in Keras)\n",
        "+ Using a smaller dataset to experiment faster (e.g. 1-10% of training samples of 10 classes of food)\n",
        "+ Data augmentation (how to make your training dataset more diverse without adding more data)\n",
        "+ Running a series of modelling experiments on our Food Vision data\n",
        "  + Model 0: a transfer learning model using the Keras Functional API\n",
        "  + Model 1: a feature extraction transfer learning model on 1% of the data with data augmentation\n",
        "  + Model 2: a feature extraction transfer learning model on 10% of the data with data augmentation\n",
        "  + Model 3: a fine-tuned transfer learning model on 10% of the data\n",
        "  + Model 4: a fine-tuned transfer learning model on 100% of the data\n",
        "+ Introduce the ModelCheckpoint callback to save intermediate training results\n",
        "+ Compare model experiments results using TensorBoard\n"
      ],
      "metadata": {
        "id": "U82K9M6FII3w"
      }
    }
  ]
}