{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMLs2W3sbob+EYIUKS4fFHZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aykq/internship/blob/main/tensorflow/04_Transfer_Learning_with_TensorFlow_Part_2_Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning with TensorFlow Part 2: Fine-tuning"
      ],
      "metadata": {
        "id": "Lwn0ugA-Hxr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous section, we saw how we could leverage feature extraction transfer learning to get far better results on our Food Vision project than building our own models (even with less data).\n",
        "\n",
        "Now we're going to cover another type of transfer learning: fine-tuning.\n",
        "\n",
        "In **fine-tuning transfer learning** the pre-trained model weights from another model are unfrozen and tweaked during to better suit your own data.\n",
        "\n",
        "For feature extraction transfer learning, you may only train the top 1-3 layers of a pre-trained model with your own data, in fine-tuning transfer learning, you might train 1-3+ layers of a pre-trained model (where the '+' indicates that many or all of the layers could be trained).\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/05-transfer-learning-feature-extraction-vs-fine-tuning.png)\n",
        "\n",
        "Feature extraction transfer learning vs. fine-tuning transfer learning. The main difference between the two is that in fine-tuning, more layers of the pre-trained model get unfrozen and tuned on custom data. This fine-tuning usually takes more data than feature extraction to be effective."
      ],
      "metadata": {
        "id": "vxfTNGJdH6tE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What we're going to cover\n",
        "\n",
        "We're going to go through the follow with TensorFlow:\n",
        "+ Introduce fine-tuning, a type of transfer learning to modify a pre-trained model to be more suited to your data\n",
        "+ Using the Keras Functional API (a differnt way to build models in Keras)\n",
        "+ Using a smaller dataset to experiment faster (e.g. 1-10% of training samples of 10 classes of food)\n",
        "+ Data augmentation (how to make your training dataset more diverse without adding more data)\n",
        "+ Running a series of modelling experiments on our Food Vision data\n",
        "  + Model 0: a transfer learning model using the Keras Functional API\n",
        "  + Model 1: a feature extraction transfer learning model on 1% of the data with data augmentation\n",
        "  + Model 2: a feature extraction transfer learning model on 10% of the data with data augmentation\n",
        "  + Model 3: a fine-tuned transfer learning model on 10% of the data\n",
        "  + Model 4: a fine-tuned transfer learning model on 100% of the data\n",
        "+ Introduce the ModelCheckpoint callback to save intermediate training results\n",
        "+ Compare model experiments results using TensorBoard\n"
      ],
      "metadata": {
        "id": "U82K9M6FII3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# issues with TensorFlow 2.10+, however, TensorFlow 2.9 seems to work better\n",
        "\n",
        "# install TensorFlow 2.9.0 (\"-U\" stands for \"update\", \"-q\" stands for \"quiet\")\n",
        "!pip install -U -q tensorflow==2.9.0\n",
        "\n",
        "import tensorflow as tf\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sbg6oan8KYvw",
        "outputId": "d6559b0c-e7bf-4826-d143-0ad01c94897a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m95.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m109.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-datasets 4.9.2 requires protobuf>=3.20, but you have protobuf 3.19.6 which is incompatible.\n",
            "tensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 3.19.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mTensorFlow version: 2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# are we using a GPU? (if not & you're using Google Colab, go to Runtime -> Change Runtime Type -> Harware Accelerator: GPU )\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6tvKlrmKsR7",
        "outputId": "253f8ca4-5669-4e5b-a6b4-e52dbfd1c484"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 18 10:44:10 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Creating helper functions\n",
        "\n",
        "Throughout your machine learning experiments, you'll likely come across snippets of code you want to use over and over again.\n",
        "\n",
        "For example, a plotting function which plots a model's history object (see plot_loss_curves() below).\n",
        "\n",
        "You could recreate these functions over and over again.\n",
        "\n",
        "But as you might've guessed, rewritting the same functions becomes tedious.\n",
        "\n",
        "One of the solutions is to store them in a helper script such as [helper_functions.py](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/helper_functions.py). And then import the necesary functionality when you need it.\n",
        "\n",
        "For example, you might write:\n",
        "\n",
        "```\n",
        "from helper_functions import plot_loss_curves\n",
        "\n",
        "...\n",
        "\n",
        "plot_loss_curves(history)\n",
        "```\n",
        "\n",
        "Let's see what this looks like."
      ],
      "metadata": {
        "id": "rzlLBUxjMDjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get helper_functions.py script from course GitHub\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "\n",
        "# import helper functions we're going to use\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, walk_through_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lG7n8vj4MR8G",
        "outputId": "b207bc41-a4e8-4d32-a5ca-f6114123b348"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-18 10:44:10--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-18 10:44:10 (106 MB/s) - ‘helper_functions.py’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wonderful, now we've got a bunch of helper functions we can use throughout the notebook without having to rewrite them from scratch each time.\n",
        "\n",
        "**Note:** If you're running this notebook in Google Colab, when it times out Colab will delete the helper_functions.py file. So to use the functions imported above, you'll have to rerun the cell.\n",
        "\n"
      ],
      "metadata": {
        "id": "_P9cYfgaNIkL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 10 Food Classes: Working with less data\n",
        "\n",
        "We saw in the previous notebook that we could get great results with only 10% of the training data using transfer learning with TensorFlow Hub.\n",
        "\n",
        "In this notebook, we're going to continue to work with smaller subsets of the data, except this time we'll have a look at how we can use the in-built pretrained models within the `tf.keras.applications` module as well as how to fine-tune them to our own custom dataset.\n",
        "\n",
        "We'll also practice using a new but similar dataloader function to what we've used before, `image_dataset_from_directory` which is part of the `tf.keras.preprocessing` module.\n",
        "\n",
        "Finally, we'll also be practicing using the [Keras Functional API](https://keras.io/guides/functional_api/) for building deep learning models. The Functional API is a more flexible way to create models than the `tf.keras.Sequential` API.\n",
        "\n",
        "We'll explore each of these in more detail as we go.\n",
        "\n",
        "Let's start by downloading some data.\n",
        "\n"
      ],
      "metadata": {
        "id": "MOoOJKA5krZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get 10% of the data of the 10 classes\n",
        "!wget https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
        "\n",
        "unzip_data(\"10_food_classes_10_percent.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2UK5uw-klc0",
        "outputId": "f203d7f0-e886-4291-e52d-40a92283e8fd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-18 10:44:11--  https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.251.167.128, 142.251.16.128, 172.253.62.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.251.167.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 168546183 (161M) [application/zip]\n",
            "Saving to: ‘10_food_classes_10_percent.zip.1’\n",
            "\n",
            "10_food_classes_10_ 100%[===================>] 160.74M   212MB/s    in 0.8s    \n",
            "\n",
            "2023-07-18 10:44:12 (212 MB/s) - ‘10_food_classes_10_percent.zip.1’ saved [168546183/168546183]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we're downloading is the 10 food classes dataset (from Food 101) with 10% of the training images we used in the previous notebook.\n",
        "\n",
        "**Note:** You can see how this dataset was created in the [image data modification notebook](https://github.com/mrdbourke/tensorflow-deep-learning/blob/main/extras/image_data_modification.ipynb).\n",
        "\n"
      ],
      "metadata": {
        "id": "mZbJpWh-mOU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# walk through 10 percent data directory and list number of files\n",
        "walk_through_dir(\"10_food_classes_10_percent\")"
      ],
      "metadata": {
        "id": "zq7YP3IgmJfj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that each of the training directories contain 75 images and each of the testing directories contain 250 images.\n",
        "\n",
        "Let's define our training and test filepaths.\n",
        "\n"
      ],
      "metadata": {
        "id": "RhHecDY5mbQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create training and test directories\n",
        "train_dir = \"10_food_classes_10_percent/train/\"\n",
        "test_dir = \"10_food_classes_10_percent/test/\""
      ],
      "metadata": {
        "id": "1ovQ4Ci6mYq9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got some image data, we need a way of loading it into a TensorFlow compatible format.\n",
        "\n",
        "Previously, we've used the `ImageDataGenerator` class. And while this works well and is still very commonly used, this time we're going to use the `image_data_from_directory` function.\n",
        "\n",
        "It works much the same way as `ImageDataGenerator`'s `flow_from_directory` method meaning your images need to be in the following file format:\n",
        "\n",
        "```\n",
        "Example of file structure\n",
        "\n",
        "10_food_classes_10_percent <- top level folder\n",
        "└───train <- training images\n",
        "│   └───pizza\n",
        "│   │   │   1008104.jpg\n",
        "│   │   │   1638227.jpg\n",
        "│   │   │   ...      \n",
        "│   └───steak\n",
        "│       │   1000205.jpg\n",
        "│       │   1647351.jpg\n",
        "│       │   ...\n",
        "│   \n",
        "└───test <- testing images\n",
        "│   └───pizza\n",
        "│   │   │   1001116.jpg\n",
        "│   │   │   1507019.jpg\n",
        "│   │   │   ...      \n",
        "│   └───steak\n",
        "│       │   100274.jpg\n",
        "│       │   1653815.jpg\n",
        "│       │   ...\n",
        "```\n",
        "\n",
        "One of the main benefits of using `tf.keras.prepreprocessing.image_dataset_from_directory()` rather than `ImageDataGenerator` is that it creates a `tf.data.Dataset` object rather than a generator. The main advantage of this is the `tf.data.Dataset` API is much more efficient (faster) than the ImageDataGenerator API which is paramount for larger datasets.\n",
        "\n",
        "Let's see it in action.\n",
        "\n"
      ],
      "metadata": {
        "id": "7zSEBq6UmnUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create data inputs\n",
        "import tensorflow as tf\n",
        "IMG_SIZE = (224, 224) # define image size\n",
        "train_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=train_dir,\n",
        "                                                                            image_size=IMG_SIZE,\n",
        "                                                                            label_mode=\"categorical\", # what type are the labels?\n",
        "                                                                            batch_size=32) # batch_size is 32 by default, this is generally a good number\n",
        "test_data_10_percent = tf.keras.preprocessing.image_dataset_from_directory(directory=test_dir,\n",
        "                                                                           image_size=IMG_SIZE,\n",
        "                                                                           label_mode=\"categorical\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HWdvj5Hmj2_",
        "outputId": "08da16c7-4b1d-48be-8a91-7aaf41b61cf2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 750 files belonging to 10 classes.\n",
            "Found 2500 files belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks like our dataloaders have found the correct number of images for each dataset.\n",
        "\n",
        "For now, the main parameters we're concerned about in the `image_dataset_from_directory()` funtion are:\n",
        "\n",
        "+ `directory` - the filepath of the target directory we're loading images in from.\n",
        "+ `image_size` - the target size of the images we're going to load in (height, width).\n",
        "+ `batch_size` - the batch size of the images we're going to load in. For example if the batch_size is 32 (the default), batches of 32 images and labels at a time will be passed to the model.\n",
        "\n",
        "There are more we could play around with if we needed to in the tf.keras.preprocessing [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory).\n",
        "\n",
        "If we check the training data datatype we should see it as a BatchDataset with shapes relating to our data."
      ],
      "metadata": {
        "id": "DbquBAG4nJQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check the training data datatype\n",
        "train_data_10_percent"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQp-cpiPnGdx",
        "outputId": "54df5606-fe59-4cf6-df81-eecbd95161e7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above output:\n",
        "\n",
        "+ `(None, 224, 224, 3)` refers to the tensor shape of our images where `None` is the batch size, `224` is the height (and width) and `3` is the color channels (red, green, blue).\n",
        "+ `(None, 10)` refers to the tensor shape of the labels where `None` is the batch size and `10` is the number of possible labels (the 10 different food classes).\n",
        "+ Both image tensors and labels are of the datatype `tf.float32`.\n",
        "\n",
        "The `batch_size` is `None` due to it only being used during model training. You can think of `None` as a placeholder waiting to be filled with the `batch_size` parameter from `image_dataset_from_directory()`.\n",
        "\n",
        "Another benefit of using the tf.data.Dataset API are the assosciated methods which come with it.\n",
        "\n",
        "For example, if we want to find the name of the classes we were working with, we could use the `class_names` attribute."
      ],
      "metadata": {
        "id": "A-ymFfiEnezH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check out the class names of our dataset\n",
        "train_data_10_percent.class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnyUzAHVnc7i",
        "outputId": "0465fa5c-5bab-431e-ffae-7aef06ae414c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chicken_curry',\n",
              " 'chicken_wings',\n",
              " 'fried_rice',\n",
              " 'grilled_salmon',\n",
              " 'hamburger',\n",
              " 'ice_cream',\n",
              " 'pizza',\n",
              " 'ramen',\n",
              " 'steak',\n",
              " 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Or if we wanted to see an example batch of data, we could use the take() method.\n",
        "\n"
      ],
      "metadata": {
        "id": "WVkzsk0Gn-zN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see an example batch of data\n",
        "for images, labels in train_data_10_percent.take(1):\n",
        "  print(images, labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s5ncsqmn8Uq",
        "outputId": "d97cd7ed-c1e8-42a4-f2e4-3951d1a8aeea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[[[106.21938    66.29081    21.219387 ]\n",
            "   [ 75.10714    33.249996    3.8775506]\n",
            "   [116.65817    71.43878    36.566334 ]\n",
            "   ...\n",
            "   [167.11252   104.03599    36.255325 ]\n",
            "   [190.95396   125.122314   56.953915 ]\n",
            "   [164.06577    94.22388    25.530066 ]]\n",
            "\n",
            "  [[ 98.90306    50.188774   10.887752 ]\n",
            "   [110.719406   58.076546   25.005117 ]\n",
            "   [178.62755   124.897964   87.255104 ]\n",
            "   ...\n",
            "   [163.55092   101.29584    24.295877 ]\n",
            "   [133.31117    73.16322     3.5102139]\n",
            "   [140.05634    79.699196   16.816578 ]]\n",
            "\n",
            "  [[163.90816   108.122444   58.556114 ]\n",
            "   [144.17857    85.52041    43.56122  ]\n",
            "   [126.86224    64.03061    21.056122 ]\n",
            "   ...\n",
            "   [168.48419   103.96381    13.775132 ]\n",
            "   [138.80093    82.67339     8.887798 ]\n",
            "   [136.74022    85.17393    28.39854  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[250.78574   238.29085   157.85202  ]\n",
            "   [251.72961   238.72961   159.94388  ]\n",
            "   [250.78572   238.12245   159.16835  ]\n",
            "   ...\n",
            "   [251.61739   226.73982   137.21426  ]\n",
            "   [252.94386   228.94386   139.37239  ]\n",
            "   [251.85709   227.85709   138.28561  ]]\n",
            "\n",
            "  [[249.92856   237.92856   153.92856  ]\n",
            "   [250.85713   238.85713   154.85713  ]\n",
            "   [249.71428   237.92856   155.8163   ]\n",
            "   ...\n",
            "   [250.        225.0153    131.96939  ]\n",
            "   [249.92856   224.92856   132.92856  ]\n",
            "   [248.        224.        134.       ]]\n",
            "\n",
            "  [[248.77042   236.77042   150.77042  ]\n",
            "   [249.33167   237.33167   151.33167  ]\n",
            "   [248.78572   237.21428   153.       ]\n",
            "   ...\n",
            "   [250.35718   225.57144   131.77559  ]\n",
            "   [250.35718   225.35718   133.35718  ]\n",
            "   [250.71436   225.71436   135.71436  ]]]\n",
            "\n",
            "\n",
            " [[[ 55.482143   47.070473   41.25893  ]\n",
            "   [ 53.468433   43.51467    46.017857 ]\n",
            "   [ 91.00734    68.86416    70.560905 ]\n",
            "   ...\n",
            "   [  5.          5.          7.       ]\n",
            "   [  5.          5.          7.       ]\n",
            "   [  5.          4.          9.       ]]\n",
            "\n",
            "  [[ 57.647003   45.188778   30.205038 ]\n",
            "   [ 62.35077    49.365112   43.71301  ]\n",
            "   [ 68.27136    45.620533   40.759563 ]\n",
            "   ...\n",
            "   [  5.9285717   5.9285717   7.9285717]\n",
            "   [  5.9285717   5.9285717   7.9285717]\n",
            "   [  5.9285717   4.9285717   9.928572 ]]\n",
            "\n",
            "  [[ 61.0676     45.57621    28.125956 ]\n",
            "   [ 59.80963    45.67283    34.834503 ]\n",
            "   [ 61.41167    42.764347   34.277424 ]\n",
            "   ...\n",
            "   [  6.          6.          8.       ]\n",
            "   [  6.          6.          8.       ]\n",
            "   [  6.          5.         10.       ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[ 27.491377   19.491377    6.4913764]\n",
            "   [ 29.058037   21.058037    8.058036 ]\n",
            "   [ 30.         22.          9.       ]\n",
            "   ...\n",
            "   [ 38.148907   34.148907   33.622173 ]\n",
            "   [ 35.103596   31.103598   32.103596 ]\n",
            "   [ 33.7229     28.722902   32.7229   ]]\n",
            "\n",
            "  [[ 29.529634   21.529634    8.529634 ]\n",
            "   [ 28.857117   20.857117    7.8571167]\n",
            "   [ 28.857117   20.857117    7.8571167]\n",
            "   ...\n",
            "   [ 34.736725   30.71982    30.352875 ]\n",
            "   [ 35.50009    30.55396    34.392353 ]\n",
            "   [ 35.983494   30.983496   34.983494 ]]\n",
            "\n",
            "  [[ 29.55468    21.55468     8.55468  ]\n",
            "   [ 30.143066   22.143066    9.143066 ]\n",
            "   [ 30.143066   22.143066    9.143066 ]\n",
            "   ...\n",
            "   [ 45.53578    41.29915    43.00905  ]\n",
            "   [ 43.221313   38.221313   42.221313 ]\n",
            "   [ 41.95767    36.95767    42.95767  ]]]\n",
            "\n",
            "\n",
            " [[[154.86734    87.22449    37.15306  ]\n",
            "   [128.54082    60.540813   15.397958 ]\n",
            "   [138.0051     70.005104   25.005102 ]\n",
            "   ...\n",
            "   [127.86219    17.219402   20.005138 ]\n",
            "   [130.2857     19.285702   25.285702 ]\n",
            "   [125.15804    14.158046   20.158047 ]]\n",
            "\n",
            "  [[152.13266    86.13265    36.132652 ]\n",
            "   [134.41837    67.42347    22.408161 ]\n",
            "   [135.30103    67.30102    22.301022 ]\n",
            "   ...\n",
            "   [126.42853    15.785736   17.       ]\n",
            "   [129.         19.         20.142857 ]\n",
            "   [127.188705   17.188704   19.525373 ]]\n",
            "\n",
            "  [[144.0153     81.22959    30.229593 ]\n",
            "   [139.16837    75.168365   29.168365 ]\n",
            "   [131.23979    64.02551    19.02551  ]\n",
            "   ...\n",
            "   [128.26018    17.617386   18.494905 ]\n",
            "   [128.58673    18.602047   17.571428 ]\n",
            "   [128.86226    19.076538   17.647966 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[198.78064   107.07143    30.698915 ]\n",
            "   [193.08673   102.14285    29.372404 ]\n",
            "   [193.66846   102.62254    32.14292  ]\n",
            "   ...\n",
            "   [219.9336    126.36212    53.71933  ]\n",
            "   [217.45404   123.45405    53.02552  ]\n",
            "   [219.1328    125.1328     55.132797 ]]\n",
            "\n",
            "  [[200.70918   109.70917    28.566286 ]\n",
            "   [195.27554   104.27554    24.989807 ]\n",
            "   [194.28574   104.08677    28.571428 ]\n",
            "   ...\n",
            "   [222.62755   125.84181    54.168404 ]\n",
            "   [219.14288   125.14288    55.142883 ]\n",
            "   [221.26022   126.260216   58.26022  ]]\n",
            "\n",
            "  [[198.87244   108.87244    22.158081 ]\n",
            "   [201.28575   110.28575    28.479563 ]\n",
            "   [197.21422   105.92342    28.576433 ]\n",
            "   ...\n",
            "   [229.92862   133.14288    61.071564 ]\n",
            "   [226.64291   129.64291    60.642914 ]\n",
            "   [224.71436   127.714355   59.714355 ]]]\n",
            "\n",
            "\n",
            " ...\n",
            "\n",
            "\n",
            " [[[  3.          3.          3.       ]\n",
            "   [  3.          3.          3.       ]\n",
            "   [  3.          3.          3.       ]\n",
            "   ...\n",
            "   [  8.642858    8.642858   10.642858 ]\n",
            "   [  8.642858    8.642858   10.642858 ]\n",
            "   [  8.          8.         10.       ]]\n",
            "\n",
            "  [[  3.          3.          3.       ]\n",
            "   [  3.          3.          3.       ]\n",
            "   [  3.          3.          3.       ]\n",
            "   ...\n",
            "   [ 10.         10.         12.       ]\n",
            "   [  9.932715    9.932715   11.932715 ]\n",
            "   [  9.928572    9.928572   11.928572 ]]\n",
            "\n",
            "  [[  3.          3.          3.       ]\n",
            "   [  3.          3.          3.       ]\n",
            "   [  3.          3.          3.       ]\n",
            "   ...\n",
            "   [ 10.599788   10.599788   12.599788 ]\n",
            "   [ 10.         10.         12.       ]\n",
            "   [  9.861284    9.861284   11.861284 ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[133.91962   126.4911    120.70536  ]\n",
            "   [137.05803   130.05803   124.05804  ]\n",
            "   [138.        131.        125.       ]\n",
            "   ...\n",
            "   [127.42853   131.40019   139.8424   ]\n",
            "   [127.42853   132.        138.64279  ]\n",
            "   [126.91989   131.49136   137.70563  ]]\n",
            "\n",
            "  [[130.60872   123.60872   117.60872  ]\n",
            "   [133.91515   126.91515   120.91515  ]\n",
            "   [134.91165   127.14826   123.43844  ]\n",
            "   ...\n",
            "   [124.62048   131.62048   137.62048  ]\n",
            "   [123.85712   130.85712   136.85712  ]\n",
            "   [122.85712   130.85712   133.85712  ]]\n",
            "\n",
            "  [[129.35268   122.35268   116.35268  ]\n",
            "   [132.4152    124.415215  121.415215 ]\n",
            "   [133.8479    125.8479    122.8479   ]\n",
            "   ...\n",
            "   [122.20506   131.20506   136.20506  ]\n",
            "   [121.3779    130.3779    135.3779   ]\n",
            "   [119.70984   129.70984   131.70984  ]]]\n",
            "\n",
            "\n",
            " [[[182.7143    168.7143    157.7143   ]\n",
            "   [186.47449   172.47449   161.47449  ]\n",
            "   [188.78572   174.78572   163.78572  ]\n",
            "   ...\n",
            "   [116.28571   109.50001    81.9286   ]\n",
            "   [118.47453   112.47453    86.47453  ]\n",
            "   [112.65818   108.65818    83.65818  ]]\n",
            "\n",
            "  [[184.2143    170.2143    157.2143   ]\n",
            "   [187.0051    173.0051    160.0051   ]\n",
            "   [189.57143   175.57143   162.57143  ]\n",
            "   ...\n",
            "   [118.        111.214294   85.64288  ]\n",
            "   [118.92859   112.92859    88.92859  ]\n",
            "   [113.38267   109.38267    84.38267  ]]\n",
            "\n",
            "  [[189.42857   175.78572   159.78572  ]\n",
            "   [189.2296    175.58673   159.58673  ]\n",
            "   [189.59694   175.95409   159.95409  ]\n",
            "   ...\n",
            "   [121.38266   113.81125    90.596954 ]\n",
            "   [123.658195  117.658195   93.658195 ]\n",
            "   [120.85205   116.85205    91.85205  ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[125.22456   111.58177    96.07152  ]\n",
            "   [117.80109   101.87251    85.658226 ]\n",
            "   [108.71938    93.38268    72.9796   ]\n",
            "   ...\n",
            "   [ 36.0918     26.188768   21.617388 ]\n",
            "   [ 34.51524    24.198963   21.785736 ]\n",
            "   [ 38.94901    28.22455    25.872547 ]]\n",
            "\n",
            "  [[119.69381   106.69381    89.69381  ]\n",
            "   [124.43877   109.43877    90.43877  ]\n",
            "   [110.15817    95.15817    74.58674  ]\n",
            "   ...\n",
            "   [ 39.443897   29.413279   27.642883 ]\n",
            "   [ 41.86735    31.857147   30.86225  ]\n",
            "   [ 39.479557   29.428528   28.596926 ]]\n",
            "\n",
            "  [[111.10191    98.10191    80.84679  ]\n",
            "   [117.484566  104.55599    85.34171  ]\n",
            "   [111.97974    97.6226     76.40831  ]\n",
            "   ...\n",
            "   [ 38.33672    25.132563   25.775446 ]\n",
            "   [ 38.709126   25.499878   27.90296  ]\n",
            "   [ 39.806297   26.046017   28.632776 ]]]\n",
            "\n",
            "\n",
            " [[[253.        255.        250.45918  ]\n",
            "   [253.        255.        250.09183  ]\n",
            "   [253.        255.        250.27551  ]\n",
            "   ...\n",
            "   [253.        255.        252.       ]\n",
            "   [253.        255.        252.       ]\n",
            "   [253.        255.        252.       ]]\n",
            "\n",
            "  [[254.        254.        254.2398   ]\n",
            "   [254.        254.        253.93367  ]\n",
            "   [254.        254.        254.08673  ]\n",
            "   ...\n",
            "   [254.        254.        252.       ]\n",
            "   [254.        254.        252.       ]\n",
            "   [254.        254.        252.       ]]\n",
            "\n",
            "  [[254.        253.21428   255.       ]\n",
            "   [254.        253.21428   255.       ]\n",
            "   [254.        253.21428   255.       ]\n",
            "   ...\n",
            "   [254.        254.        254.       ]\n",
            "   [254.        254.        254.       ]\n",
            "   [254.        254.        254.       ]]\n",
            "\n",
            "  ...\n",
            "\n",
            "  [[254.14288   254.64285   250.42853  ]\n",
            "   [253.85716   254.92857   250.42853  ]\n",
            "   [254.00002   254.78572   250.42853  ]\n",
            "   ...\n",
            "   [252.83165   255.        250.       ]\n",
            "   [253.08669   255.        250.       ]\n",
            "   [252.57643   255.        250.       ]]\n",
            "\n",
            "  [[253.95407   254.04593   252.05103  ]\n",
            "   [253.93365   254.06635   252.01021  ]\n",
            "   [253.94386   254.05614   252.03062  ]\n",
            "   ...\n",
            "   [253.05614   253.94386   255.       ]\n",
            "   [253.06635   253.93365   255.       ]\n",
            "   [253.04593   253.95407   255.       ]]\n",
            "\n",
            "  [[251.45915   254.77042   254.58676  ]\n",
            "   [251.09183   254.95409   254.40309  ]\n",
            "   [251.2755    254.86226   254.49492  ]\n",
            "   ...\n",
            "   [254.86226   254.        251.28564  ]\n",
            "   [254.95407   254.        251.28564  ]\n",
            "   [254.7704    254.        251.28564  ]]]], shape=(32, 224, 224, 3), dtype=float32) tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]], shape=(32, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice how the image arrays come out as tensors of pixel values where as the labels come out as one-hot encodings (e.g. `[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]` for `hamburger`).\n",
        "\n"
      ],
      "metadata": {
        "id": "md77RINvoNLT"
      }
    }
  ]
}