{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOvxp4QCaTWpwRgTCXKbOMg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aykq/internship/blob/main/tensorflow/05_Transfer_Learning_with_TensorFlow_Part_3_Scaling_up_(Food_Vision_mini).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 05 - Transfer Learning with TensorFlow Part 3: Scaling up (🍔👁 Food Vision mini)"
      ],
      "metadata": {
        "id": "uaSOE3tkB7sF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous two notebooks ([transfer learning part 1: feature extraction](https://github.com/aykq/internship/blob/main/tensorflow/03_Transfer_Learning_with_TensorFlow_Part_1_Feature_Extraction.ipynb) and [part 2: fine-tuning](https://github.com/aykq/internship/blob/main/tensorflow/04_Transfer_Learning_with_TensorFlow_Part_2_Fine_tuning.ipynb)) we've seen the power of transfer learning.\n",
        "\n",
        "Now we know our smaller modelling experiments are working, it's time to step things up a notch with more data.\n",
        "\n",
        "This is a common practice in machine learning and deep learning: get a model working on a small amount of data before scaling it up to a larger amount of data.\n",
        "\n",
        "**Note:** You haven't forgotten the machine learning practitioners motto have you? \"Experiment, experiment, experiment.\"\n",
        "\n",
        "It's time to get closer to our Food Vision project coming to life. In this notebook we're going to scale up from using 10 classes of the Food101 data to using all of the classes in the Food101 dataset.\n",
        "\n",
        "**Our goal is to beat the original [Food101 paper](https://data.vision.ee.ethz.ch/cvl/datasets_extra/food-101/static/bossard_eccv14_food-101.pdf)'s results with 10% of data.**\n",
        "\n",
        "![](https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/images/06-ml-serial-experimentation.png)\n",
        "*Machine learning practitioners are seriel experimenters. Start small, get a model working, see if your experiments work then gradually scale them up where you want to go (we're going to be looking at scaling up throughout this notebook).*"
      ],
      "metadata": {
        "id": "-Y0VK1mPCS71"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# What we're going to cover\n",
        "\n",
        "We're going to go through the follow with TensorFlow:\n",
        "\n",
        "+ Downloading and preparing 10% of the Food101 data (10% of training data)\n",
        "+ Training a feature extraction transfer learning model on 10% of the Food101 training data\n",
        "+ Fine-tuning our feature extraction model\n",
        "+ Saving and loaded our trained model\n",
        "+ Evaluating the performance of our Food Vision model trained on 10% of the training data\n",
        "  + Finding our model's most wrong predictions\n",
        "+ Making predictions with our Food Vision model on custom images of food\n"
      ],
      "metadata": {
        "id": "piIZGmRDC78q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2m4V3r1-B5Ib",
        "outputId": "7c99da05-8bd3-4212-f7ae-c12a15183d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 18 12:51:22 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   52C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# are we using a GPU?\n",
        "# if not, and you're in Google Colab, go to Runtime -> Change runtime type -> Hardware accelerator -> GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# Creating helper functions\n",
        "\n",
        "We've created a series of helper functions throughout the previous notebooks. Instead of rewriting them (tedious), we'll import the `helper_functions.py` file from GitHub page."
      ],
      "metadata": {
        "id": "NeQpcAwCDM-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get helper functions file\n",
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbjdeukYDKY8",
        "outputId": "12bb05f1-9447-4627-8781-74bc4aa1b5ea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-18 12:51:23--  https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 10246 (10K) [text/plain]\n",
            "Saving to: ‘helper_functions.py.1’\n",
            "\n",
            "\rhelper_functions.py   0%[                    ]       0  --.-KB/s               \rhelper_functions.py 100%[===================>]  10.01K  --.-KB/s    in 0s      \n",
            "\n",
            "2023-07-18 12:51:23 (72.2 MB/s) - ‘helper_functions.py.1’ saved [10246/10246]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, unzip_data, compare_historys, walk_through_dir"
      ],
      "metadata": {
        "id": "u0c2DwzoDcLG"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}